{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/needitem/HIT/blob/master/Server.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bEv2MiNl-pL",
        "outputId": "4b22c20f-ecd8-4c5f-b2a0-7a9a7abda7da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HairFastGAN'...\n",
            "remote: Enumerating objects: 525, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 525 (delta 5), reused 3 (delta 3), pack-reused 516\u001b[K\n",
            "Receiving objects: 100% (525/525), 2.21 MiB | 5.46 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n",
            "/content/HairFastGAN\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AIRI-Institute/HairFastGAN\n",
        "%cd HairFastGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNcBPs1ImA0-",
        "outputId": "a0bcc881-6f07-4d1a-faf2-f07a7f6d9bcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-28 09:17:05--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240528%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240528T091705Z&X-Amz-Expires=300&X-Amz-Signature=8f3b28ae7bddce93b88dd793d82c042eae127266a6bdb0b5e37cff9ab0f871b2&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-05-28 09:17:05--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240528%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240528T091705Z&X-Amz-Expires=300&X-Amz-Signature=8f3b28ae7bddce93b88dd793d82c042eae127266a6bdb0b5e37cff9ab0f871b2&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77854 (76K) [application/octet-stream]\n",
            "Saving to: ‘ninja-linux.zip’\n",
            "\n",
            "ninja-linux.zip     100%[===================>]  76.03K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2024-05-28 09:17:06 (961 KB/s) - ‘ninja-linux.zip’ saved [77854/77854]\n",
            "\n",
            "Archive:  ninja-linux.zip\n",
            "  inflating: /usr/local/bin/ninja    \n",
            "update-alternatives: using /usr/local/bin/ninja to provide /usr/bin/ninja (ninja) in auto mode\n"
          ]
        }
      ],
      "source": [
        "# Install ninja - small build system to run C++, C\n",
        "# Install Ninja - small build system https://github.com/ninja-build/ninja\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZpE6HLemCzg",
        "outputId": "2da2e18d-bdd4-407b-96c0-9c2274d9c46e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fpie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install lib\n",
        "!pip install pyngrok streamlit pillow==10.0.0 face_alignment dill==0.2.7.1 addict fpie git+https://github.com/openai/CLIP.git -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtVlbK6PmD0L",
        "outputId": "d54b3c23-c43d-4240-d137-842a15bd18eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.3.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchvision) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTCOmbNvmFFJ",
        "outputId": "05692c06-c74b-415f-8689-0a386a9681f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HairFastGAN'...\n",
            "remote: Enumerating objects: 67, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 67 (delta 4), reused 0 (delta 0), pack-reused 1 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (67/67), 7.75 MiB | 5.99 MiB/s, done.\n",
            "Filtering content: 100% (34/34), 7.20 GiB | 128.14 MiB/s, done.\n",
            "Encountered 1 file(s) that should have been pointers, but weren't:\n",
            "\tdocs/assets/logo.webp\n"
          ]
        }
      ],
      "source": [
        "# Download pretrain\n",
        "!git clone https://huggingface.co/AIRI-Institute/HairFastGAN\n",
        "!cd HairFastGAN && git lfs pull && cd ..\n",
        "!mv HairFastGAN/pretrained_models pretrained_models\n",
        "!mv HairFastGAN/input input\n",
        "!rm -rf HairFastGAN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from hair_swap import HairFast, get_parser\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import torchvision.transforms as T\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cGFcyFgg97Kf",
        "outputId": "42adfcfd-585f-4c27-e6ea-dca41f57717f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_YDi5wJjuAQ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('../captured'):\n",
        "    os.makedirs('../captured')\n",
        "if not os.path.exists('../output'):\n",
        "    os.makedirs('../output')\n",
        "if not os.path.exists('../shape'):\n",
        "    os.makedirs('../shape')\n",
        "if not os.path.exists('../color'):\n",
        "    os.makedirs('../color')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQU7-X8knvfU",
        "outputId": "559cf3e5-0b0e-4536-93f0-3b1de075036e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading StyleGAN2 from checkpoint: pretrained_models/StyleGAN/ffhq.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n",
            "100%|██████████| 44.7M/44.7M [00:01<00:00, 43.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading e4e over the pSp framework from checkpoint: pretrained_models/encoder4editing/e4e_ffhq_encode.pt\n",
            "Network [SPADEGenerator] was created. Total number of parameters: 266.9 million. To see the architecture, do print(network).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 335M/335M [00:05<00:00, 62.5MiB/s]\n"
          ]
        }
      ],
      "source": [
        "model_args = get_parser()\n",
        "hair_fast = HairFast(model_args.parse_args([]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aYjT5Xyta4o",
        "outputId": "beb9ad76-b72c-4544-85cc-053e3ca58c76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://8a27-35-234-62-95.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading StyleGAN2 from checkpoint: pretrained_models/StyleGAN/ffhq.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [28/May/2024 09:38:59] \"GET /send_pic HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading e4e over the pSp framework from checkpoint: pretrained_models/encoder4editing/e4e_ffhq_encode.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [28/May/2024 09:39:05] \"GET /send_pic HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network [SPADEGenerator] was created. Total number of parameters: 266.9 million. To see the architecture, do print(network).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [28/May/2024 09:39:10] \"GET /send_pic HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/May/2024 09:39:10] \"GET /send_pic HTTP/1.1\" 200 -\n",
            "Starting alignment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of faces detected: 1\n",
            "Number of faces detected: 1\n",
            "Number of faces detected: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/HairFastGAN/models/FeatureStyleEncoder/pixel2style2pixel/models/stylegan2/model.py:274: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  out = F.conv2d(input, weight, padding=self.padding, groups=batch)\n",
            "/content/HairFastGAN/models/FeatureStyleEncoder/pixel2style2pixel/models/stylegan2/model.py:259: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  out = F.conv_transpose2d(input, weight, padding=0, stride=2, groups=batch)\n",
            "/content/HairFastGAN/models/stylegan2/model.py:260: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  out = F.conv_transpose2d(input, weight, padding=0, stride=2, groups=batch)\n",
            "/content/HairFastGAN/models/sean_codes/models/pix2pix_model.py:140: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "  input_label = self.FloatTensor(bs, nc, h, w).zero_()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/instancenorm.py:80: UserWarning: input's size at dim=1 does not match num_features. You can silence this warning by not passing in num_features, which is not used because affine=False\n",
            "  warnings.warn(f\"input's size at dim={feature_dim} does not match num_features. \"\n",
            "/content/HairFastGAN/models/stylegan2/model.py:275: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  out = F.conv2d(input, weight, padding=self.padding, groups=batch)\n",
            "INFO:werkzeug:127.0.0.1 - - [28/May/2024 09:39:30] \"GET /generate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/May/2024 09:39:57] \"GET /send_pic HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/May/2024 09:40:03] \"GET /send_pic HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/May/2024 09:40:08] \"GET /send_pic HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [28/May/2024 09:40:41] \"POST /get_pic HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pyngrok import ngrok\n",
        "from werkzeug.utils import secure_filename\n",
        "from flask import Flask, request, jsonify, send_file\n",
        "from PIL import Image\n",
        "from HairFastGAN.hair_swap import HairFast, get_parser\n",
        "from HairFastGAN.models.Blending import Blending\n",
        "from torchvision.transforms import ToPILImage\n",
        "import io, sys\n",
        "from base64 import encodebytes\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from werkzeug.exceptions import BadRequest\n",
        "\n",
        "# Ngrok Setup\n",
        "NGROK_AUTH_TOKEN = \"2h3DfFNrOUbylnCMvRu08g66GQ7_2bj5sKfoCw9emtkoNyWUt\"  # Replace with your actual token\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "http_tunnel = ngrok.connect(5000)\n",
        "print(\"Public URL:\", http_tunnel.public_url)\n",
        "\n",
        "# Flask App\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Configuration\n",
        "output_path = \"../output\"\n",
        "input_path = \"../captured\"\n",
        "name = \"taeho\"\n",
        "\n",
        "Blending_checkpoint = \"Default\"\n",
        "Alignment_images = \"Auto\"\n",
        "\n",
        "hair_fast_instans = None\n",
        "path_to_imgs = {}\n",
        "\n",
        "\n",
        "# Model Initialization\n",
        "def initialize_models():\n",
        "    global hair_fast_instans\n",
        "    model_args = get_parser()\n",
        "    hair_fast = HairFast(model_args.parse_args([]))\n",
        "    hair_fast_instans = {'Default': hair_fast}\n",
        "\n",
        "\n",
        "def convert_input(path_inp, inp):\n",
        "    path = os.path.join(path_inp, inp)\n",
        "    try:\n",
        "        if os.path.isfile(path):\n",
        "            if path in path_to_imgs:\n",
        "                return path_to_imgs[path]\n",
        "            else:\n",
        "                path_to_imgs[path] = Image.open(path)\n",
        "                return path_to_imgs[path]\n",
        "    except Exception as e:\n",
        "        print(f\"Can't open the image {inp}\")\n",
        "        print(e)\n",
        "        return False\n",
        "\n",
        "def get_image_files(directory_path):\n",
        "    image_file_names = []\n",
        "    for file_name in os.listdir(directory_path):\n",
        "        file_path = os.path.join(directory_path, file_name)\n",
        "        if os.path.isfile(file_path) and file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
        "            image_file_names.append(file_name)\n",
        "    return image_file_names\n",
        "\n",
        "def get_response_image(file_path):\n",
        "    # 이미지 파일을 읽어와서 PNG 형식으로 변환하여 Base64로 인코딩하여 반환\n",
        "    with open(file_path, \"rb\") as image_file:\n",
        "        image = Image.open(image_file)\n",
        "        buffered = BytesIO()\n",
        "        image.save(buffered, format=\"PNG\")\n",
        "        encoded_image = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
        "    return encoded_image\n",
        "\n",
        "@app.route(\"/get_pic\", methods=['POST'])\n",
        "def get_pic():\n",
        "    try:\n",
        "        if 'files' not in request.files:\n",
        "            return jsonify({\"error\": \"No files part in the request\"}), 400\n",
        "\n",
        "        files = request.files.getlist('files')\n",
        "        if not files:\n",
        "            return jsonify({\"error\": \"No files found in the request\"}), 400\n",
        "\n",
        "        required_files = {\"face.png\", \"target.png\", \"color.png\"}  # Define the expected files\n",
        "        uploaded_files = {file.filename for file in files}  # Set of uploaded filenames\n",
        "\n",
        "        if not required_files.issubset(uploaded_files):\n",
        "            missing_files = required_files - uploaded_files\n",
        "            return jsonify({\"error\": f\"Missing required files: {', '.join(missing_files)}\"}), 400\n",
        "\n",
        "        # Create the subdirectory for the user (if it doesn't exist)\n",
        "        user_directory = os.path.join(face_path, name)\n",
        "        os.makedirs(user_directory, exist_ok=True)\n",
        "\n",
        "        for file in files:\n",
        "            if file.filename == '':\n",
        "                return jsonify({\"error\": \"One of the files has an empty filename\"}), 400\n",
        "\n",
        "            filename = secure_filename(file.filename)\n",
        "            file_path = os.path.join(user_directory, filename)  # Save within the user's directory\n",
        "\n",
        "            # Save temporarily, then convert and save permanently\n",
        "            temp_path = os.path.join(user_directory, 'temp_' + filename)\n",
        "            file.save(temp_path)\n",
        "            with Image.open(temp_path) as img:\n",
        "                img = img.convert(\"RGB\")\n",
        "                img.save(file_path)\n",
        "            os.remove(temp_path)\n",
        "\n",
        "        app.logger.debug(f\"Saved files to: {user_directory}\")\n",
        "\n",
        "        return jsonify({\"message\": \"Images uploaded successfully\"}), 200\n",
        "\n",
        "    except BadRequest as e:\n",
        "        # Handle Bad Requests (e.g., invalid file names) more specifically\n",
        "        return jsonify({\"error\": str(e)}), e.code\n",
        "    except Exception as e:\n",
        "        app.logger.error(f\"An error occurred: {str(e)}\")\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route(\"/send_pic\", methods=[\"GET\"])\n",
        "def send_pic():\n",
        "    try:\n",
        "        path = os.path.join(os.getcwd(), \"input\")\n",
        "        files = os.listdir(path)\n",
        "        if not files:\n",
        "            return jsonify({\"error\": \"No files available\"}), 404\n",
        "\n",
        "        encoded_images = []\n",
        "        for file in files:\n",
        "            file_path = os.path.join(path, file)\n",
        "            if os.path.isfile(file_path):\n",
        "                encoded_image = get_response_image(file_path)\n",
        "                encoded_images.append(encoded_image)\n",
        "\n",
        "        if not encoded_images:\n",
        "            return jsonify({\"error\": \"No files available\"}), 404\n",
        "\n",
        "        return jsonify({\"images\": encoded_images})\n",
        "\n",
        "    except Exception as e:\n",
        "        app.logger.error(f\"An error occurred: {str(e)}\")\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route(\"/generate\", methods=['GET'])\n",
        "def generate():\n",
        "    global hair_fast_instans\n",
        "    if hair_fast_instans is None:\n",
        "        initialize_models()\n",
        "\n",
        "    try:\n",
        "        # Image Paths (using provided directory and name)\n",
        "        face_image_path = os.path.join(input_path, name, \"face.png\")\n",
        "        shape_image_path = os.path.join(input_path, name, \"target.png\")\n",
        "        color_image_path = os.path.join(input_path, name, \"color.png\")\n",
        "\n",
        "        # Load Images\n",
        "        face = Image.open(face_image_path)\n",
        "        shape = Image.open(shape_image_path)\n",
        "        color = Image.open(color_image_path)\n",
        "\n",
        "        # Hair Swap and Blending (with error handling)\n",
        "        if Blending_checkpoint not in hair_fast_instans:\n",
        "            raise ValueError(f\"Invalid Blending checkpoint: {Blending_checkpoint}\")\n",
        "\n",
        "        need_alignment = any(img.size != (1024, 1024) for img in [face, shape, color])\n",
        "        if Alignment_images == 'On' or (Alignment_images == 'Auto' and need_alignment):\n",
        "            print(\"Starting alignment\", file=sys.stderr)\n",
        "            result_image, *_ = hair_fast_instans[Blending_checkpoint](face, shape, color, align=True)\n",
        "            result_image = ToPILImage()(result_image)\n",
        "        else:\n",
        "            result_image = hair_fast_instans[Blending_checkpoint](face, shape, color)\n",
        "\n",
        "        # Save and Send Result (with error handling)\n",
        "        result_image_path = os.path.join(output_path, 'result_image.png')\n",
        "        result_image.save(result_image_path)\n",
        "        return send_file(result_image_path, mimetype='image/png')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in generate route: {e}\")\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == '__main__':\n",
        "    app.run(port=5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OLwJqf6hj0TG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}